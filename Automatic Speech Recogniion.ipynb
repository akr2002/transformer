{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d252e78-1c0d-486c-bee2-5bb5decf6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:100]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f86642-85f3-4dba-9c8d-a9d95a93ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b09ede34-05e3-4df2-bc47-d4c1f1e5c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542f4f7b-229a-49a8-a774-fef7aaf76178",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.remove_columns([\"english_transcription\", \"intent_class\", \"lang_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc91e69f-27da-443d-ab38-2b1679b2af87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/b63b156957c7ca71c29ec1b1c70b7c01f82e37338e69d6b00d0e88dc4d3d3ee4/en-US~JOINT_ACCOUNT/602bac2e963e11ccd901cda4.wav',\n",
       " 'audio': {'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/b63b156957c7ca71c29ec1b1c70b7c01f82e37338e69d6b00d0e88dc4d3d3ee4/en-US~JOINT_ACCOUNT/602bac2e963e11ccd901cda4.wav',\n",
       "  'array': array([ 0.        ,  0.        , -0.00024414, ..., -0.00024414,\n",
       "          0.00048828, -0.00024414]),\n",
       "  'sampling_rate': 8000},\n",
       " 'transcription': 'hi I was wondering how I would set up a joint account with my partner'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10b37c50-56c3-4333-ac4a-de3385d0b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 15:04:10.084142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-10 15:04:10.401331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 15:04:10.495470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 15:04:10.527669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 15:04:10.725498: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 15:04:12.145724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768409b070b4c2a8ee77749fd396b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/dev/transformer/venv/lib/python3.12/site-packages/transformers/configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f05fb8aef82497abb0a79644f222e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0392cb78d61e4bc2b67a707a9e94b37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc20ab-99c4-4668-9ea2-3afbd34477f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740d0a25-3e1c-4322-9ff4-16f536921a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/b63b156957c7ca71c29ec1b1c70b7c01f82e37338e69d6b00d0e88dc4d3d3ee4/en-US~JOINT_ACCOUNT/602bac2e963e11ccd901cda4.wav',\n",
       " 'audio': {'path': '/home/user/.cache/huggingface/datasets/downloads/extracted/b63b156957c7ca71c29ec1b1c70b7c01f82e37338e69d6b00d0e88dc4d3d3ee4/en-US~JOINT_ACCOUNT/602bac2e963e11ccd901cda4.wav',\n",
       "  'array': array([-2.99292151e-06,  5.17739973e-05,  4.47832281e-06, ...,\n",
       "          2.52652564e-04, -2.09215432e-04, -3.20373743e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'hi I was wondering how I would set up a joint account with my partner'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae06b7b-d533-4947-af00-e32f4b8edf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase(example):\n",
    "    return {\"transcription\": example[\"transcription\"].upper()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ee2692-1cc4-4ada-9bc6-31aa0cfb43dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c953727dba4e06bc5ed76acec952b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9ef8436cf145c1aa43b00ad4828d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minds = minds.map(uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c613e5f-cc4a-47d5-80be-a5aa19590c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n",
    "    batch[\"input_length\"] =  len(batch[\"input_values\"][0])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1d4e170-c89c-4ef6-b6ba-56d3fa9a0fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06791a202199496fbd08375d0d07612e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0077d10db6f243968e0614b2c7b0df12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7022b82-bebb-44a3-8a97-82d19ba3621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e9e9c0-2fa1-45c7-95ca-84a8870c5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPaddnig:\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(input, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        labels_batch = self.processor.pad(labels=label, padding=self.padding, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"inputs_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bb26f43-6bc4-47fd-be12-720a34d6bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPaddnig(processor=processor, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e8cb74-587f-48f1-b78c-5d04c8e7c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ad7b7eb8f4409fbb5ccf29ba052ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "wer = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37a5a662-920c-404f-b934-7c0042066999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91c67c64-97c7-45d7-a9bf-27a2e72fab71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Wav2Vec2Processor' object has no attribute 'tokenzier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCTC, TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     ctc_loss_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenzier\u001b[49m\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Wav2Vec2Processor' object has no attribute 'tokenzier'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCTC, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenzier.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b291a2e-70fb-45f8-9b0a-c4f6deff5c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
